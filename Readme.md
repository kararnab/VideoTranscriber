# ğŸ™ï¸ Local GPU Transcription with Faster-Whisper (Docker)

A fully local, GPU-accelerated transcription pipeline using **faster-whisper**, packaged in Docker so you donâ€™t have to fight Python, CUDA, or dependency hell.

## âœ¨ Highlights
```
âœ… Runs entirely on your machine (100% local & offline)
âœ… GPU-accelerated (5â€“10Ã— faster than CPU)
âœ… Supports Whisper large-v3
âœ… Dockerized (no Python / CUDA pain)
âœ… Works great for long videos
```

## ğŸ§  Who Is This For?

This section helps people self-identify (very important for engagement):

- ğŸ“š Students transcribing lectures
- ğŸ‘¨â€ğŸ’» Engineers watching system design talks
- ğŸ¥ YouTubers & course creators
- ğŸ”’ Anyone working with sensitive or private audio
- ğŸ¤– People building LLM pipelines on top of transcripts

If you watch **long videos and take notes**, this saves hours.

## ğŸš€ What This Does

- Transcribes audio/video files using Whisper models
- Uses your NVIDIA GPU for fast inference
- Outputs clean, timestamped transcripts

## ğŸ§© Requirements
### Hardware
- NVIDIA GPU (tested with RTX series)
- ~12 GB VRAM recommended for large-v3

### Software
- Docker Desktop
- NVIDIA GPU Driver
- NVIDIA Container Toolkit

## ğŸ› ï¸ Getting Started

### Build the Docker Image (Once)

```bash
docker build -t whisper-local -f docker/Dockerfile docker
```
This creates a reusable image with:

- Ubuntu 22.04
- CUDA runtime
- Python 3.11
- faster-whisper
- FFmpeg

### Run Transcription
#### Windows Cmd Prompt
```bash
docker run --rm --gpus all -v "%cd%\input:/input" -v "%cd%\output:/output" whisper-local transcribe.py /input/video.mp4 /output/video.txt
```
#### Windows Power shell
```bash
docker run --rm --gpus all -v "${PWD}\input:/input" -v "${PWD}\output:/output" -v "${PWD}\model_cache:/app/model_cache" whisper-local transcribe.py /input/video.mp4 /output/video.txt
```

#### Linux / macOS
```bash
docker run --rm --gpus all \
  -v "$(pwd)/input:/input" \
  -v "$(pwd)/output:/output" \
  whisper-local transcribe.py /input/video.mp4 /output/video.txt
```

After completion, youâ€™ll find:
```
transcripts/video.txt
```
### Output Example
```
[00:00:03] In this video, weâ€™re going to talk about distributed systemsâ€¦
[00:02:41] A common mistake engineers make is assuming consistencyâ€¦
[00:07:12] Letâ€™s walk through a real-world architecture exampleâ€¦
```
Clean. Timestamped. Ready for:

- Search
- Notes
- Summaries
- LLM pipelines

## Troubleshoot

### Verify GPU usage
To confirm the container is using your GPU:
```bash
nvidia-smi
```
You should see activity during transcription.

## ğŸ”’ Privacy & Offline Use

- Your audio/video never leaves your machine
- Models are downloaded once from Hugging Face
- After that, transcription works fully offline

You may see this warning on first run:
`Warning: You are sending unauthenticated requests to the HF Hub`

This is **normal** and **safe**.

## ğŸ§  Designed for Knowledge Pipelines

This project is the first stage of a larger workflow:
```
Local
â”œâ”€â”€ Transcription (completed)
â”œâ”€â”€ Transcript cleanup (remove fillers, noise)
â””â”€â”€ Chunking

Cloud / LLM
â”œâ”€â”€ Note extraction
â”œâ”€â”€ Summaries
â””â”€â”€ Insights
```
The goal is to evolve this into a knowledge extraction pipeline, not just a transcription script.

## ğŸš§ Project Status

### Current state
- Fully local, GPU-accelerated transcription using faster-whisper
- Dockerized setup (no Python / CUDA dependency issues)
- Works with any audio or video file
- Outputs clean, timestamped transcripts

What this project does today
- ğŸ§ Audio/Video â†’ Text (fast, local, private)
- ğŸ“„ Produces transcripts suitable for downstream processing (notes, summaries, search)

### ğŸ”® Future Direction (Planned)

- Batch transcription for entire folders
- Markdown-formatted outputs
- Transcript chunking for long videos
- LLM-based note extraction and summarization
- Optional local vs cloud summarization workflows

The goal is to evolve this from a transcription tool into a knowledge extraction pipeline.

### Promt Engineering
You are a senior engineer.
Convert this transcript into:
1. Key ideas
2. Step-by-step explanation
3. Practical takeaways
4. Common mistakes or warnings
5. A short summary

[Your_Transcript_here]

## ğŸ¤ Contributing

If this saved you time:

â­ Star the repo

ğŸ§  Share it with someone who watches long videos

ğŸ› ï¸ Open a PR or issue